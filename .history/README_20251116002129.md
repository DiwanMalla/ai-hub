# AI Model Hub

AI Model Hub is a lightweight Next.js (App Router) site to collect, preview and interactively test AI models. The hub supports both browser-based inference (using Transformers.js with WebAssembly/WebGPU) and server-side inference (using Hugging Face Inference API) depending on the model type.

This repository includes example integrations:
- **BRIA RMBG 1.4** - Browser-based background removal using Transformers.js
- **BRIA FIBO** - Server-side text-to-image generation with structured JSON prompts

---

## Features

- **Centralized model configuration** (`src/config/models.ts`) to add new models quickly.
- **Flexible playground components**:
  - `ModelTryPanel.tsx` - For browser-based models (image segmentation, classification, etc.)
  - `FiboPanel.tsx` - For text-to-image models with Generate, Refine, and Inspire modes
- **Dynamic model pages** under `/models/[slug]` with model metadata, docs links and live playground.
- **Responsive, sticky navigation** that works on all screen sizes with accessible mobile menu.
- **Remote image proxy** (`/api/image-proxy`) to fetch remote images server-side and avoid CORS issues.
- **Server-side inference endpoints** for gated models and models requiring GPU acceleration.

---

## Getting started (development)

Requirements
- Node.js 18+ recommended
- pnpm / npm (examples use `npm`)

Install dependencies

```bash
npm install
```

Start dev server

```bash
npm run dev
```

Open http://localhost:3000 in your browser.

Notes about running models in the browser
- Transformers.js runs model artifacts in the browser using WASM or WebGPU. Some models may require additional files or be gated (private) on Hugging Face; such models will fail to load in the browser unless you have proper access or use server-side execution.
- For the best in-browser performance enable WebGPU in a supported browser (Chromium-based browsers with WebGPU enabled). Transformers.js will fall back to WASM when hardware acceleration is not available.

---

## Add a new model

1. Open `src/config/models.ts` and add a new `ModelConfig` entry. Provide `pipeline.task`, `pipeline.model` and UI metadata.
2. Commit the changes. The homepage will list the new model and the `Try now` button will link to `/models/{slug}`.
3. If your model requires extra preprocessor or files that are private on Hugging Face, either make the files public or run the model server-side and add a server endpoint to proxy inference results.

## Project layout

- `src/config/models.ts` - central model definitions
- `src/components/ModelTryPanel.tsx` - reusable interactive playground (client)
- `src/app/page.tsx` - homepage that lists models
- `src/app/models/[slug]/page.tsx` - dynamic model page
- `src/app/api/image-proxy/route.ts` - serverless route to fetch remote images and return data-URIs

## Deployment

You can deploy this Next.js site to Vercel, Netlify, or any platform that supports Next.js App Router. For Vercel, push to GitHub and import the repo; Vercel will pick up the Next.js settings.

If you expect large model assets to be fetched from Hugging Face, consider either hosting assets in a CDN or running server-side inference and exposing a slim API.

## Troubleshooting

- "Unauthorized access" while loading model files: some models/files on Hugging Face are gated and require credentials. If you hit this, either use a public model or run server-side with an authenticated Hugging Face token.
- CORS errors when loading remote images: use the `/api/image-proxy` endpoint (already wired to the playground) to avoid browser CORS restrictions.
- Slow model loads: the first-time load downloads model artifacts ‚Äî subsequent in-memory loads are faster. Use smaller models for demos.

## Contributing

Contributions are welcome. Add new models to `src/config/models.ts` and open a PR. Please include model license/source links in `links.huggingface` and `links.documentation` when available.

# AI Model Hub

A modern platform for testing and integrating AI models, supporting both browser-based inference (Transformers.js) and server-side generation (Hugging Face Inference API).

## Featured Models

- **BRIA RMBG 1.4** - Production-grade background removal running in your browser
- **BRIA FIBO** - JSON-native text-to-image generation with structured prompts

---

## ‚ú® Features

- **Centralized model configuration** (`src/config/models.ts`) - Add new models quickly
- **Flexible playground components**:
  - `ModelTryPanel.tsx` - Browser-based models (segmentation, classification, etc.)
  - `FiboPanel.tsx` - Text-to-image with Generate, Refine, and Inspire modes
- **Dynamic model pages** under `/models/[slug]` with metadata and live playgrounds
- **Responsive design** with sticky navigation and accessible mobile menu
- **Remote image proxy** to avoid CORS issues
- **Server-side inference** for gated models and GPU-intensive tasks

---

## üöÄ Quick Start

### Prerequisites
- Node.js 18+ recommended
- Modern browser with WebGPU support for browser-based models

### Installation

```bash
npm install
```

### Environment Setup

For **BRIA FIBO** and other gated models, you need a Hugging Face API token:

1. Get your token from [Hugging Face Settings](https://huggingface.co/settings/tokens)
2. Request access to gated models (e.g., [BRIA FIBO](https://huggingface.co/briaai/FIBO))
3. Create `.env.local` in the project root:

```bash
HUGGINGFACE_API_TOKEN=hf_xxxYOUR_TOKEN_HERExxx
```

**‚ö†Ô∏è Never commit `.env.local` to version control!**

### Development

```bash
npm run dev
```

Open http://localhost:3000 in your browser.

---

## üß© Project Structure

```
src/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ models.ts                    # Model definitions
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ ModelTryPanel.tsx           # Browser-based inference UI
‚îÇ   ‚îî‚îÄ‚îÄ FiboPanel.tsx               # Text-to-image UI
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                    # Homepage
‚îÇ   ‚îú‚îÄ‚îÄ models/[slug]/page.tsx      # Dynamic model pages
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ image-proxy/route.ts    # CORS proxy for remote images
‚îÇ       ‚îú‚îÄ‚îÄ rmbg/route.ts           # RMBG server endpoint
‚îÇ       ‚îî‚îÄ‚îÄ fibo/route.ts           # FIBO inference endpoint
```

---

## ü§ñ Model Details

### BRIA RMBG 1.4
- **Type**: Image Segmentation (Background Removal)
- **Execution**: Browser (Transformers.js + WebGPU)
- **Privacy**: All processing happens locally‚Äîimages never leave your device
- **License**: `bria-rmbg-1.4` (non-commercial). [Commercial licensing available](https://bria.ai)

### BRIA FIBO
- **Type**: Text-to-Image Generation
- **Execution**: Server-side (Hugging Face Inference API)
- **Features**:
  - **Generate**: Expand short prompts into 1,000+ word structured JSON
  - **Refine**: Targeted edits (lighting, camera, composition) without prompt drift
  - **Inspire**: Generate variations from reference images
- **License**: Gated model - requires Hugging Face token and access approval
- **Paper**: [arXiv:2511.06876](https://arxiv.org/abs/2511.06876)

---

## üìù Adding New Models

1. Open `src/config/models.ts` and add a new `ModelConfig`:

```typescript
{
  id: "your-model-id",
  slug: "your-model-slug",
  name: "Your Model Name",
  category: "Task Type",
  pipeline: {
    task: "image-segmentation", // or "text-to-image", etc.
    model: "org/model-name",
  },
  // ... more config
}
```

2. The homepage will automatically list the new model
3. For text-to-image models, create a server API route in `src/app/api/`
4. For browser models, ensure they're compatible with Transformers.js

---

## üåê Deployment

### Vercel (Recommended)

1. Push to GitHub
2. Import in Vercel
3. Add `HUGGINGFACE_API_TOKEN` to Environment Variables
4. Deploy

### Other Platforms

- **Netlify**: Add env vars in site settings
- **Docker**: Set env vars in container runtime
- **Custom**: Ensure Node.js 18+ and set environment variables

---

## üîß Troubleshooting

### "Unauthorized access" errors
- Gated models require a Hugging Face token with proper access
- Ensure `HUGGINGFACE_API_TOKEN` is set in your environment
- Request access on the model's Hugging Face page

### CORS errors with remote images
- Use the `/api/image-proxy` endpoint (already wired in the UI)
- The proxy fetches images server-side and returns data URLs

### Slow model loads (browser models)
- First load downloads model artifacts (can take 1-2 minutes)
- Subsequent loads use cached artifacts and are much faster
- Enable WebGPU in your browser for best performance

### Model loading failures
- Check browser console for specific errors
- Verify model is compatible with Transformers.js
- Consider switching to server-side inference for large models

---

## üõ†Ô∏è Tech Stack

- **Next.js 16** - React framework with App Router
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling
- **Transformers.js** - Browser-based ML inference
- **WebGPU/WebAssembly** - Hardware acceleration
- **Hugging Face Inference API** - Server-side generation

---

## ü§ù Contributing

Contributions welcome! To add models:

1. Fork the repository
2. Add model config to `src/config/models.ts`
3. Create UI component if needed
4. Test locally
5. Submit PR with model license info and links

---

## üìÑ License & Credits

This repository is provided as-is. Individual models have their own licenses:
- **BRIA RMBG 1.4**: `bria-rmbg-1.4` license
- **BRIA FIBO**: Gated - check [model card](https://huggingface.co/briaai/FIBO)

**Credits:**
- [Transformers.js](https://github.com/xenova/transformers.js) by Xenova
- [Hugging Face](https://huggingface.co) for the ML ecosystem
- [BRIA AI](https://bria.ai) for RMBG and FIBO models

---

## üìö Resources

- [Transformers.js Docs](https://huggingface.co/docs/transformers.js)
- [Next.js Documentation](https://nextjs.org/docs)
- [Hugging Face Models](https://huggingface.co/models)
- [FIBO Paper](https://arxiv.org/abs/2511.06876)
- [BRIA AI](https://bria.ai)

